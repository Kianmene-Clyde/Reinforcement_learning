# ğŸ¤– Projet Apprentissage par Renforcement (Reinforcement Learning)

## ğŸ¯ Objectif pÃ©dagogique

Ce projet vise Ã  :

- ImplÃ©menter et comparer des **algorithmes fondamentaux d'apprentissage par renforcement (RL)**.
- Appliquer ces agents Ã  plusieurs **environnements conÃ§us sur mesure** (LineWorld, GridWorld, Monty Hall, etc.).
- Ã‰tudier l'**impact des hyperparamÃ¨tres** sur les performances des politiques apprises.
- Identifier les **meilleures stratÃ©gies** pour chaque environnement.
- Savoir quand et pourquoi utiliser chaque mÃ©thode (DP, Monte Carlo, TD, Planning).

---

## ğŸ§© Structure du projet

```
RL_Project/
â”‚
â”œâ”€â”€ agents/                   # Tous les algorithmes RL implÃ©mentÃ©s
â”‚   â”œâ”€â”€ dynamic_programming.py
â”‚   â”œâ”€â”€ monte_carlo_methods.py
â”‚   â”œâ”€â”€ temporal_difference_methods.py
â”‚   â””â”€â”€ planning_methods.py
â”‚
â”œâ”€â”€ environments/             # Tous les environnements RL (y compris secrets)
â”‚   â”œâ”€â”€ line_world_env.py
â”‚   â”œâ”€â”€ grid_world_env.py
â”‚   â”œâ”€â”€ monty_hall_lv1_env.py
â”‚   â”œâ”€â”€ monty_hall_lv2_env.py
â”‚   â”œâ”€â”€ rps_game_env.py
â”‚   â””â”€â”€ secret_env_*.py       # (fournis plus tard)
â”‚
â”œâ”€â”€ utils/                    # Fonctions auxiliaires
â”‚   â”œâ”€â”€ export_results.py     # Export Excel
â”‚   â”œâ”€â”€ save_load_policy.py   # Sauvegarde/chargement de politiques
â”‚   â””â”€â”€ visualize_results.py  # GÃ©nÃ©ration de graphes
â”‚
â”œâ”€â”€ main.py                   # Menu principal Pygame (agent vs humain)
â”œâ”€â”€ experiments.py            # Teste tous les agents sur tous les environnements
â”œâ”€â”€ Reports/                  # Exports .xlsx + visualisations
â””â”€â”€ README.md                 # Ce fichier
```

---

## ğŸ§  Agents implÃ©mentÃ©s

| CatÃ©gorie           | MÃ©thodes                                                                      |
|---------------------|-------------------------------------------------------------------------------|
| Dynamic Programming | `policy_iteration`, `value_iteration`                                         |
| Monte Carlo         | `on_policy_first_visit_mc_control`, `monte_carlo_es`, `off_policy_mc_control` |
| Temporal Difference | `sarsa`, `q_learning`, `expected_sarsa` (optionnel)                           |
| Planning            | `dyna_q`, `dyna_q_plus` (optionnel)                                           |

---

## ğŸŒ Environnements simulÃ©s

| Nom             | Description courte                                                  |
|-----------------|---------------------------------------------------------------------|
| LineWorld       | Environnement linÃ©aire terminal                                     |
| GridWorld       | Grille 2D avec Ã©tats terminaux                                      |
| Monty Hall LV1  | ProblÃ¨me classique des 3 portes, choix initial puis switch          |
| Monty Hall LV2  | Variante Ã  5 portes avec 4 Ã©tapes                                   |
| RPS Game        | Jeu Pierre-Feuille-Ciseaux sur 2 tours, avec adversaire stratÃ©gique |
| SecretEnv (0â€“3) | Environnements mystÃ¨res fournis par l'enseignant en fin de projet   |

âœ… Tous les environnements incluent :

- une **interface Pygame**
- un mode **manuel (humain)** et **automatique (agent entraÃ®nÃ©)**
- une **visualisation pas-Ã -pas**
- un affichage **des flÃ¨ches de politique** ou **rÃ©sultats interactifs**

---

## âš™ï¸ MÃ©thodologie expÃ©rimentale

- Tous les **agents sont testÃ©s sur tous les environnements**
- Une **grille d'hyperparamÃ¨tres** est explorÃ©e automatiquement (`experiments.py`)
- Les **scores moyens et paramÃ¨tres optimaux** sont exportÃ©s dans `Reports/global_comparison.xlsx`
- Un script de **visualisation CLI** permet de gÃ©nÃ©rer tous les **graphes utiles** (boxplots, heatmaps, courbes,
  corrÃ©lations)
- Les **politiques apprises sont sauvegardables** et **rÃ©utilisables**

---

## ğŸ’¾ Sauvegarde / Chargement de politiques

```python
from Utils.save_load_policy import save_policy, load_policy

# Sauvegarder
save_policy(policy, "policy_gridworld_qlearning.pkl")

# Charger
policy = load_policy("policy_gridworld_qlearning.pkl")
```

â¡ï¸ Permet de **rejouer une stratÃ©gie sans rÃ©entraÃ®nement** (utile pour la soutenance).

---

## ğŸ“Š Visualisation des rÃ©sultats

Lancer :

```bash
python utils/visualize_results.py
```

Permet de gÃ©nÃ©rer :

- ğŸ“¦ Boxplots par agent et environnement
- ğŸ”¥ Heatmaps hyperparamÃ¨tres (alpha vs epsilon, etc.)
- ğŸ“ˆ Courbes de convergence (score vs Ã©pisodes)
- ğŸ” CorrÃ©lation entre hyperparamÃ¨tres et performance

Toutes les images sont stockÃ©es dans `Reports/Visualisations/`.

---

## ğŸ§  InterprÃ©tation des rÃ©sultats

| Agent              | Environnements prÃ©fÃ©rÃ©s        | Explication                                         |
|--------------------|--------------------------------|-----------------------------------------------------|
| `policy_iteration` | `line_world`, `grid_world`     | converge vite avec modÃ¨le parfait                   |
| `q_learning`       | `monty_hall_lv1`, `rps_game`   | efficace avec exploration contrÃ´lÃ©e                 |
| `dyna_q_plus`      | `monty_hall_lv2`, `grid_world` | avantage avec planification et exploration diffÃ©rÃ©e |

---

## ğŸ“ Soutenance recommandÃ©e

**Slides Ã  inclure :**

- ğŸ¯ Objectif du projet
- ğŸ§  MÃ©thodologie (exploration de lâ€™espace des hyperparamÃ¨tres)
- ğŸ“ˆ RÃ©sultats comparÃ©s (tableaux + graphes)
- âœ… DÃ©monstration live via `main.py` (politique sauvegardÃ©e)
- ğŸ¤” InterprÃ©tation et perspectives

---

## ğŸ“¦ Contenu Ã  rendre

- âœ… Code source
- âœ… Rapport final avec graphiques
- âœ… Fichier `.xlsx` des scores et paramÃ¨tres
- âœ… Slides de soutenance
- âœ… Politiques sauvegardÃ©es
- âœ… Screenshots si nÃ©cessaire

---

## ğŸ‘¨â€ğŸ« Contact pÃ©dagogique

- **Nom** : Nicolas VIDAL
- **Email** : [nvidal@myges.fr](mailto:nvidal@myges.fr)

---

# ğŸ§ª RÃ©sumÃ© des RÃ©sultats des ExpÃ©riences RL

## ğŸ” Objectif

Comparer les performances de diffÃ©rents agents RL sur plusieurs environnements avec variation d'hyperparamÃ¨tres pour
identifier les meilleures combinaisons.

---

## âœ… Meilleurs Couples Agent / Environnement

| Environnement  | Agent Optimal    | Score Moyen | HyperparamÃ¨tres  |
|----------------|------------------|-------------|------------------|
| LineWorld      | Policy Iteration | XX.XX       | gamma=0.99, ...  |
| GridWorld      | Value Iteration  | XX.XX       | gamma=0.9, ...   |
| Monty Hall LV1 | Q-learning       | XX.XX       | epsilon=0.2, ... |
| Monty Hall LV2 | Dyna-Q+          | XX.XX       | planning=10, ... |
| RPS Game       | Expected SARSA   | XX.XX       | alpha=0.5, ...   |

> *Ces rÃ©sultats sont extraits automatiquement de `BestParams` gÃ©nÃ©rÃ© par `experiments.py`.*

---

## ğŸ“ˆ Analyse Visuelle Automatique

Lance `visualize_results.py` pour :

- GÃ©nÃ©rer des **boxplots** par agent/environnement
- GÃ©nÃ©rer des **heatmaps** des hyperparamÃ¨tres (alpha vs epsilon)
- Tracer des **courbes score vs Ã©pisodes**
- Calculer la **corrÃ©lation** entre hyperparamÃ¨tres et performances

---

## ğŸ—‚ï¸ Recommandations

- Reutilise les politiques optimales sauvegardÃ©es (`Reports/Policies/*.pkl`)
- Priorise les algorithmes par type dâ€™environnement :
    - DP pour environnements Ã  faible complexitÃ©
    - MC/TD pour environnement stochastique
    - Dyna pour environnements nÃ©cessitant du planning

---

## ğŸ“¦ Export

- RÃ©sultats complets : `Reports/global_comparison.xlsx`
- Graphiques : `Reports/Visuals/*.png`
- Politiques entraÃ®nÃ©es : `Reports/Policies/*.pkl`

---

ğŸ“ *Document Ã  intÃ©grer dans votre rapport de soutenance ou slides PowerPoint pour faciliter lâ€™analyse et
lâ€™interprÃ©tation des rÃ©sultats.*